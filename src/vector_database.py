import json
import chromadb 
import numpy as np
from typing import List, Dict
import re
from sentence_transformers import SentenceTransformer
import os

class MedicalVectorDB:
    def __init__(self, data_path: str = "data/drugs_database.json", 
                 model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ª–µ–∫–∞—Ä—Å—Ç–≤
        –ò—Å–ø–æ–ª—å–∑—É–µ–º multilingual –º–æ–¥–µ–ª—å –∫–æ—Ç–æ—Ä–∞—è –ª—É—á—à–µ –ø–æ–Ω–∏–º–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π
        """
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
        try:
            self.model = SentenceTransformer(model_name)
            print("–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
        
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection = self.client.get_or_create_collection(
            name="medical_drugs",
            metadata={"description": "Medical drugs database"}
        )
        self.data_path = data_path
        self.drugs_data = self._load_data()
    
    def _load_data(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data['–ª–µ–∫–∞—Ä—Å—Ç–≤–∞'])} –ª–µ–∫–∞—Ä—Å—Ç–≤")
            return data['–ª–µ–∫–∞—Ä—Å—Ç–≤–∞']
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return []

    def _create_semantic_drug_text(self, drug: Dict) -> str:
        # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_mapping = {
            '–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å': '–º–∏–≥—Ä–µ–Ω—å —Ü–µ—Ñ–∞–ª–≥–∏—è –≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å –≥–æ–ª–æ–≤–∞',
            '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞': '–ª–∏—Ö–æ—Ä–∞–¥–∫–∞ –∂–∞—Ä –≥–∏–ø–µ—Ä—Ç–µ—Ä–º–∏—è –≥–æ—Ä—è—á–∫–∞',
            '–≤–æ—Å–ø–∞–ª–µ–Ω–∏–µ': '–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω–æ–µ',
            '–∞—Ä—Ç—Ä–∏—Ç': '–∞—Ä—Ç—Ä–∏—Ç —Å—É—Å—Ç–∞–≤—ã —Å—É—Å—Ç–∞–≤–Ω–∞—è –±–æ–ª—å —Ä–µ–≤–º–∞—Ç–æ–∏–¥–Ω—ã–π',
            '–±–æ–ª—å': '–±–æ–ª–µ–≤–æ–π —Å–∏–Ω–¥—Ä–æ–º –±–æ–ª–µ–∑–Ω–µ–Ω–Ω–æ—Å—Ç—å –∞–Ω–∞–ª—å–≥–µ—Ç–∏–∫ –æ–±–µ–∑–±–æ–ª–∏–≤–∞—é—â–µ–µ',
            '–ª–∏—Ö–æ—Ä–∞–¥–∫–∞': '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∂–∞—Ä –ª–∏—Ö–æ—Ä–∞–¥–∫–∞ —Ñ–µ–±—Ä–∏–ª—å–Ω—ã–π',
            '–∂–∞—Ä–æ–ø–æ–Ω–∏–∂–∞—é—â–µ–µ': '–∂–∞—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ª–∏—Ö–æ—Ä–∞–¥–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ–ª–∏—Ö–æ—Ä–∞–¥–æ—á–Ω–æ–µ',
            '–æ–±–µ–∑–±–æ–ª–∏–≤–∞—é—â–µ–µ': '–±–æ–ª—å –∞–Ω–∞–ª—å–≥–µ—Ç–∏–∫ –∞–Ω–µ—Å—Ç–µ–∑–∏—è –ø—Ä–æ—Ç–∏–≤–æ–±–æ–ª–µ–≤–æ–µ',
            '–ø—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω–æ–µ': '–≤–æ—Å–ø–∞–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω—Ç–∏—Ñ–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ',
            '–∞–ª–ª–µ—Ä–≥–∏—è': '–∞–ª–ª–µ—Ä–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–∫—Ü–∏—è –∑—É–¥ —Å—ã–ø—å –∞–Ω—Ç–∏–≥–∏—Å—Ç–∞–º–∏–Ω–Ω–æ–µ',
            '–∫–∞—à–µ–ª—å': '–∫–∞—à–µ–ª—å –±—Ä–æ–Ω—Ö–∏—Ç –æ—Ç—Ö–∞—Ä–∫–∏–≤–∞—é—â–µ–µ',
            '–Ω–∞—Å–º–æ—Ä–∫': '—Ä–∏–Ω–∏—Ç –Ω–∞–∑–∞–ª—å–Ω—ã–π –∑–∞–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å –Ω–æ—Å–∞'
        }
        # –†–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∫–∞–∑–∞–Ω–∏—è —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        enhanced_indications = []
        for indication in drug['–ø–æ–∫–∞–∑–∞–Ω–∏—è']:
            enhanced = indication
            for keyword, synonyms in keyword_mapping.items():
                if keyword in indication.lower():
                    enhanced += " " + synonyms
            enhanced_indications.append(enhanced)
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        category_keywords = {
            '–∞–Ω–∞–ª—å–≥–µ—Ç–∏–∫': '–æ–±–µ–∑–±–æ–ª–∏–≤–∞—é—â–µ–µ –±–æ–ª—å –∞–Ω–∞–ª—å–≥–µ—Ç–∏–∫ –∞–Ω–µ—Å—Ç–µ–∑–∏—è',
            '–ø—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω–æ–µ': '–≤–æ—Å–ø–∞–ª–µ–Ω–∏–µ –ø—Ä–æ—Ç–∏–≤–æ–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω—ã–π –∞—Ä—Ç—Ä–∏—Ç',
            '–∞–Ω—Ç–∏–±–∏–æ—Ç–∏–∫': '–∞–Ω—Ç–∏–±–∏–æ—Ç–∏–∫ –∏–Ω—Ñ–µ–∫—Ü–∏—è –±–∞–∫—Ç–µ—Ä–∏–∏ –∞–Ω—Ç–∏–±–∞–∫—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ',
            '–∞–Ω—Ç–∏–≥–∏—Å—Ç–∞–º–∏–Ω–Ω–æ–µ': '–∞–ª–ª–µ—Ä–≥–∏—è –∞–Ω—Ç–∏–≥–∏—Å—Ç–∞–º–∏–Ω–Ω–æ–µ –∑—É–¥ —Å—ã–ø—å',
            '–∂–∞—Ä–æ–ø–æ–Ω–∏–∂–∞—é—â–µ–µ': '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∂–∞—Ä –ª–∏—Ö–æ—Ä–∞–¥–∫–∞ –∂–∞—Ä–æ–ø–æ–Ω–∏–∂–∞—é—â–µ–µ'
        }
        
        category_synonyms = category_keywords.get(drug.get('–∫–∞—Ç–µ–≥–æ—Ä–∏—è', ''), '')
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
        text_parts = [
            f"–õ–µ–∫–∞—Ä—Å—Ç–≤–æ: {drug['–Ω–∞–∑–≤–∞–Ω–∏–µ']}",
            f"–î–µ–π—Å—Ç–≤–∏–µ: {drug['–æ–ø–∏—Å–∞–Ω–∏–µ']}",
            f"–õ–µ—á–∏—Ç: {', '.join(enhanced_indications)}", 
            f"–°–∏–º–ø—Ç–æ–º—ã: {', '.join(drug['–ø–æ–∫–∞–∑–∞–Ω–∏—è'])}",
            f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {drug.get('–∫–∞—Ç–µ–≥–æ—Ä–∏—è', '')} {category_synonyms}",
            f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: {drug['–¥–æ–∑–∏—Ä–æ–≤–∫–∞']}",
            f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: {', '.join(drug['–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è'])}",
            f"–ü–æ–±–æ—á–Ω—ã–µ: {', '.join(drug['–ø–æ–±–æ—á–Ω—ã–µ_—ç—Ñ—Ñ–µ–∫—Ç—ã'])}"
        ]
        
        return ". ".join(text_parts)

    def build_vector_database(self):
        """–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
        if not self.drugs_data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–∞–∑—ã")
            return
                
        documents = []
        metadatas = []
        ids = []
        
        for drug in self.drugs_data:
            drug_text = self._create_semantic_drug_text(drug)
            documents.append(drug_text)
            
            metadata = {
                "–Ω–∞–∑–≤–∞–Ω–∏–µ": drug["–Ω–∞–∑–≤–∞–Ω–∏–µ"],
                "–∫–∞—Ç–µ–≥–æ—Ä–∏—è": drug.get("–∫–∞—Ç–µ–≥–æ—Ä–∏—è", "–Ω–µ —É–∫–∞–∑–∞–Ω–∞"),
                "–ø–æ–∫–∞–∑–∞–Ω–∏—è": ", ".join(drug["–ø–æ–∫–∞–∑–∞–Ω–∏—è"][:3]),
                "id": str(drug["id"])
            }
            metadatas.append(metadata)
            ids.append(f"drug_{drug['id']}")
        
        print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = self.model.encode(documents, normalize_embeddings=True)
        
        try:
            self.client.delete_collection("medical_drugs")
        except:
            pass
            
        self.collection = self.client.get_or_create_collection(
            name="medical_drugs",
            metadata={"description": "Medical drugs database"}
        )
        
        self.collection.add(
            embeddings=embeddings.tolist(),
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∞! –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} –∑–∞–ø–∏—Å–µ–π")
    
    def search_drugs(self, query: str, n_results: int = 5, category_filter: str = None):
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞
        """
        expanded_query = self._expand_search_query(query)
        print(f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}' -> '{expanded_query}'")
        
        where_filter = {"–∫–∞—Ç–µ–≥–æ—Ä–∏—è": category_filter} if category_filter else None
        
        try:
            query_embedding = self.model.encode([expanded_query], normalize_embeddings=True)[0].tolist()
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_filter
            )
            
            return self._format_results(results)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _expand_search_query(self, query: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
        query_expansions = {
            '–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å': '–º–∏–≥—Ä–µ–Ω—å —Ü–µ—Ñ–∞–ª–≥–∏—è',
            '–º–∏–≥—Ä–µ–Ω—å': '–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å',
            '–±–æ–ª—å': '–±–æ–ª–µ–≤–æ–π —Å–∏–Ω–¥—Ä–æ–º',
            '–±–æ–ª–∏—Ç': '–±–æ–ª—å',
            
            '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞': '–ª–∏—Ö–æ—Ä–∞–¥–∫–∞ –∂–∞—Ä',
            '–ª–∏—Ö–æ—Ä–∞–¥–∫–∞': '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∂–∞—Ä',
            '–∂–∞—Ä': '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ª–∏—Ö–æ—Ä–∞–¥–∫–∞',
            
            '–≤–æ—Å–ø–∞–ª–µ–Ω–∏–µ': '–≤–æ—Å–ø–∞–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å',
            '–∞—Ä—Ç—Ä–∏—Ç': '—Å—É—Å—Ç–∞–≤—ã –∞—Ä—Ç—Ä–∞–ª–≥–∏—è',
            
            '–∞–ª–ª–µ—Ä–≥–∏—è': '–∞–ª–ª–µ—Ä–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–∫—Ü–∏—è –∑—É–¥ —Å—ã–ø—å',
            '–∑—É–¥': '–∞–ª–ª–µ—Ä–≥–∏—è —Å—ã–ø—å',
            '—Å—ã–ø—å': '–∞–ª–ª–µ—Ä–≥–∏—è –∑—É–¥ –∫—Ä–∞–ø–∏–≤–Ω–∏—Ü–∞',
            '–∫—Ä–∞–ø–∏–≤–Ω–∏—Ü–∞': '–∞–ª–ª–µ—Ä–≥–∏—è –∑—É–¥ —Å—ã–ø—å',
            
            '–¥–∏–∞—Ä–µ—è': '–ø–æ–Ω–æ—Å –∂–∏–¥–∫–∏–π —Å—Ç—É–ª',
            '—Ç–æ—à–Ω–æ—Ç–∞': '—Ä–≤–æ—Ç–∞ –¥–∏—Å–ø–µ–ø—Å–∏—è',
            '–∏–∑–∂–æ–≥–∞': '–∂–µ–ª—É–¥–æ–∫ –≥–∞—Å—Ç—Ä–∏—Ç',
        }
        
        original_query = query.lower().strip()
        expanded_terms = set(original_query.split())
        
        for term, expansion in query_expansions.items():
            if term in original_query:
                new_terms = expansion.split()
                for new_term in new_terms:
                    if new_term not in expanded_terms:
                        expanded_terms.add(new_term)
        
        final_terms = list(expanded_terms)
        if len(final_terms) > 8:
            original_terms = original_query.split()
            final_terms = original_terms + [t for t in final_terms if t not in original_terms][:8-len(original_terms)]
        
        expanded_query = " ".join(final_terms)
        
        if expanded_query != original_query:
            print(f"üîç –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{original_query}' -> '{expanded_query}'")
        
        return expanded_query
    
    def _format_results(self, results) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        formatted_results = []
        
        if results['documents']:
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )):
                drug_id = int(metadata['id'])
                full_drug_data = next((drug for drug in self.drugs_data if drug['id'] == drug_id), None)
                
                if full_drug_data:
                    similarity = max(0.0, 1.0 - distance)
                    
                    result = {
                        "—Ä–µ–π—Ç–∏–Ω–≥": i + 1,
                        "—Å—Ö–æ–∂–µ—Å—Ç—å": f"{similarity:.3f}",
                        "–ª–µ–∫–∞—Ä—Å—Ç–≤–æ": full_drug_data["–Ω–∞–∑–≤–∞–Ω–∏–µ"],
                        "–æ–ø–∏—Å–∞–Ω–∏–µ": full_drug_data["–æ–ø–∏—Å–∞–Ω–∏–µ"],
                        "–ø–æ–∫–∞–∑–∞–Ω–∏—è": full_drug_data["–ø–æ–∫–∞–∑–∞–Ω–∏—è"],
                        "–¥–æ–∑–∏—Ä–æ–≤–∫–∞": full_drug_data["–¥–æ–∑–∏—Ä–æ–≤–∫–∞"],
                        "–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è": full_drug_data["–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è"][:3],
                        "–ø–æ–ª–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ": full_drug_data
                    }
                    formatted_results.append(result)
        
        formatted_results.sort(key=lambda x: float(x['—Å—Ö–æ–∂–µ—Å—Ç—å']), reverse=True)
        return formatted_results

def test_improved_search():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    db = MedicalVectorDB()
    
    db.build_vector_database()
    
    print("\n" + "="*60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ü–û–ò–°–ö–ê")
    print("="*60)
    
    test_cases = [
        ("–≥–æ–ª–æ–≤–Ω–∞—è –±–æ–ª—å", ["–ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª", "–ò–±—É–ø—Ä–æ—Ñ–µ–Ω", "–ê—Å–ø–∏—Ä–∏–Ω"]),
        ("—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ª–∏—Ö–æ—Ä–∞–¥–∫–∞", ["–ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª", "–ò–±—É–ø—Ä–æ—Ñ–µ–Ω", "–ê—Å–ø–∏—Ä–∏–Ω"]),
        ("–≤–æ—Å–ø–∞–ª–µ–Ω–∏–µ –∞—Ä—Ç—Ä–∏—Ç", ["–ò–±—É–ø—Ä–æ—Ñ–µ–Ω", "–ö–µ—Ç–æ—Ä–æ–ª–∞–∫"]),
        ("–∞–ª–ª–µ—Ä–≥–∏—è –∑—É–¥", ["–¶–µ—Ç–∏—Ä–∏–∑–∏–Ω", "–õ–æ—Ä–∞—Ç–∞–¥–∏–Ω"]),
        ("–∫–æ—Å–º–∏—á–µ—Å–∫–∏–µ –∫–æ—Ä–∞–±–ª–∏", [])  # –ù–µ –¥–æ–ª–∂–Ω–æ –Ω–∞–π—Ç–∏
    ]
    
    for query, expected in test_cases:
        print(f"\n–ó–∞–ø—Ä–æ—Å: '{query}'")
        print(f"   –û–∂–∏–¥–∞–µ–º—ã–µ: {expected}")
        
        results = db.search_drugs(query, n_results=5)
        found_drugs = [result['–ª–µ–∫–∞—Ä—Å—Ç–≤–æ'] for result in results]
        
        print(f"–ù–∞–π–¥–µ–Ω–æ: {found_drugs}")
        
        if expected:
            matches = [drug for drug in expected if drug in found_drugs]
            if matches:
                print(f"–ù–∞–π–¥–µ–Ω—ã –æ–∂–∏–¥–∞–µ–º—ã–µ: {matches}")
            else:
                print(f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –æ–∂–∏–¥–∞–µ–º—ã–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞")
                
                if found_drugs:
                    print(f"–í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {found_drugs[:3]}")
        else:
            if not found_drugs:
                print("–û–ö: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            else:
                print(f"–ù–∞–π–¥–µ–Ω—ã –ª–µ–∫–∞—Ä—Å—Ç–≤–∞: {found_drugs[:3]}")
        
        for result in results[:3]:
            print(f"      {result['–ª–µ–∫–∞—Ä—Å—Ç–≤–æ']}: {result['—Å—Ö–æ–∂–µ—Å—Ç—å']}")

if __name__ == "__main__":
    test_improved_search()